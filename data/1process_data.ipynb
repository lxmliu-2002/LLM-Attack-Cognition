{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225fe8ed",
   "metadata": {},
   "source": [
    "Converted `xstest_prompts.csv` to `xstest.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608178b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed! Wrote 450 records to /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/xstest.jsonl\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "csv_file = current_dir / \"ori\" / \"xstest_prompts.csv\"\n",
    "jsonl_file = current_dir / \"jsonl\" / \"xstest.jsonl\"\n",
    "\n",
    "\n",
    "id_counter = 0\n",
    "with open(csv_file, mode='r', encoding='utf-8') as csv_f, \\\n",
    "     open(jsonl_file, mode='w', encoding='utf-8') as jsonl_f:\n",
    "\n",
    "    reader = csv.DictReader(csv_f)\n",
    "\n",
    "    for row in reader:\n",
    "        if 'prompt' not in row or 'label' not in row:\n",
    "            print(f\"Warning: Missing 'prompt' or 'label' in row: {row}\")\n",
    "            continue\n",
    "\n",
    "        output_dict = {\n",
    "            'id': id_counter,\n",
    "            'prompt': row['prompt'],\n",
    "            'label': row['label']\n",
    "        }\n",
    "\n",
    "        jsonl_f.write(json.dumps(output_dict, ensure_ascii=False) + '\\n')\n",
    "        id_counter += 1\n",
    "\n",
    "print(f\"Conversion completed! Wrote {id_counter} records to {jsonl_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d6a34",
   "metadata": {},
   "source": [
    "Converted `benign_questions.csv` and `safebench.csv` to `FigStep.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94048734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully combined questions into '/mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl/FigStep.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "\n",
    "base_dir = current_dir / \"ori\" / \"FigStep\"\n",
    "output_file = current_dir / \"jsonl\" / \"FigStep.jsonl\"\n",
    "\n",
    "benign_csv = base_dir / \"benign_questions.csv\"\n",
    "unsafe_csv = base_dir / \"safebench.csv\"\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "with open(output_file, mode='w', encoding='utf-8') as jsonl_f:\n",
    "    with open(benign_csv, mode='r', encoding='utf-8-sig') as f: # 文件编码错误\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if 'question' not in row:\n",
    "                print(f\"Warning: Missing 'question' in benign_questions.csv row: {row}\")\n",
    "                continue\n",
    "            output_dict = {\n",
    "                'id': id_counter,\n",
    "                'prompt': row['question'],\n",
    "                'label': 'safe'\n",
    "            }\n",
    "            jsonl_f.write(json.dumps(output_dict, ensure_ascii=False) + '\\n')\n",
    "            id_counter += 1\n",
    "\n",
    "    with open(unsafe_csv, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if 'question' not in row:\n",
    "                print(f\"Warning: Missing 'question' in safebench.csv row: {row}\")\n",
    "                continue\n",
    "            output_dict = {\n",
    "                'id': id_counter,\n",
    "                'prompt': row['question'],\n",
    "                'label': 'unsafe'\n",
    "            }\n",
    "            jsonl_f.write(json.dumps(output_dict, ensure_ascii=False) + '\\n')\n",
    "            id_counter += 1\n",
    "\n",
    "print(f\"Successfully combined questions into '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e64df",
   "metadata": {},
   "source": [
    "Converted `jailbreak_prompts_2023_05_07.csv` to `In-The-Wild.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed! Wrote 666 records to /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl/In-The-Wild.jsonl\n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "import json\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "csv_path = current_dir / \"ori\" / \"jailbreak_prompts_2023_05_07.csv\"\n",
    "jsonl_path = current_dir / \"jsonl\" / \"In-The-Wild.jsonl\"\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "with open(csv_path, mode='r', encoding='utf-8-sig', newline='') as csv_f, \\\n",
    "        open(jsonl_path, mode='w', encoding='utf-8') as jsonl_f:\n",
    "\n",
    "    reader = csv.DictReader(csv_f)\n",
    "\n",
    "    if 'prompt' not in reader.fieldnames:\n",
    "        raise ValueError(f\"CSV file must contain 'prompt' column. Found columns: {reader.fieldnames}\")\n",
    "\n",
    "    for row in reader:\n",
    "        output_dict = {\n",
    "            'id': id_counter,\n",
    "            'prompt': row['prompt'],\n",
    "            'label': 'unsafe'\n",
    "        }\n",
    "\n",
    "        jsonl_f.write(json.dumps(output_dict, ensure_ascii=False) + '\\n')\n",
    "        id_counter += 1\n",
    "\n",
    "print(f\"Conversion completed! Wrote {id_counter} records to {jsonl_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6abe84",
   "metadata": {},
   "source": [
    "Converted `mma-output.json` to `MMA.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10eaa0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed! Wrote 1000 records to /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl/MMA.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"/mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/ori/mma-output.json\"\n",
    "jsonl_path = \"/mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl/MMA.jsonl\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as f_out:\n",
    "    for _, item in enumerate(data):\n",
    "        new_item = {\n",
    "            \"id\": id_counter,\n",
    "            \"prompt\": item[\"clean_prompt\"],\n",
    "            \"ori_prompt\": item[\"original_prompt\"],\n",
    "            \"label\": \"unsafe\"\n",
    "        }\n",
    "        id_counter += 1\n",
    "        f_out.write(json.dumps(new_item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Conversion completed! Wrote {id_counter} records to {jsonl_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91d01c",
   "metadata": {},
   "source": [
    "使用 GPT 生成 1300 条描述居家生活场景的句子，作为 safe 的补充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1300 records to /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl/gpt.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "api_key = 'sk-37O4E6xvq0PThorlCiLWnVf4FFtcv0VTJPokECKVIT3S3FGC'\n",
    "api_base_url = \"https://api.feidaapi.com/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=api_base_url\n",
    ")\n",
    "\n",
    "def sanitize_prompt(prompt):\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        system_message = (\n",
    "            \"You are a creative assistant tasked with generating sentences that depict the beauty of home life. \"\n",
    "            \"Each sentence should be no longer than 20 words and convey a sense of warmth, comfort, or happiness.\"\n",
    "        )\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=30,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        sanitized_prompt = completion.choices[0].message.content.strip()\n",
    "        return sanitized_prompt\n",
    "        \n",
    "\n",
    "def generate_new_sentences(count):\n",
    "    new_data = []\n",
    "    for i in tqdm(range(count), desc=\"Generating safe sentences\"):\n",
    "        prompt = \"Generate a sentence depicting the beauty of home life.\"\n",
    "        sentence = sanitize_prompt(prompt)\n",
    "        new_data.append({\n",
    "            \"id\": i,\n",
    "            \"original_text\": sentence,\n",
    "            \"label\": \"safe\"\n",
    "        })\n",
    "    return new_data\n",
    "\n",
    "def write_to_jsonl(data, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Saved {len(data)} records to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    output_jsonl_path = \"/mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl/gpt.jsonl\"\n",
    "\n",
    "    new_data = generate_new_sentences(1000)\n",
    "    \n",
    "    write_to_jsonl(new_data, output_jsonl_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4073c30",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4d0461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据统计结果：\n",
      "+-------------------+--------+----------+--------+\n",
      "|      文件名       |  safe  |  unsafe  |  总计  |\n",
      "+===================+========+==========+========+\n",
      "|   FigStep.jsonl   |  300   |   500    |  800   |\n",
      "+-------------------+--------+----------+--------+\n",
      "| In-The-Wild.jsonl |   0    |   666    |  666   |\n",
      "+-------------------+--------+----------+--------+\n",
      "|     MMA.jsonl     |   0    |   1000   |  1000  |\n",
      "+-------------------+--------+----------+--------+\n",
      "|     gpt.jsonl     |  1300  |    0     |  1300  |\n",
      "+-------------------+--------+----------+--------+\n",
      "|   xstest.jsonl    |  250   |   200    |  450   |\n",
      "+-------------------+--------+----------+--------+\n",
      "|       总计        |  1850  |   2366   |  4216  |\n",
      "+-------------------+--------+----------+--------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "folder_path = \"/mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl\"\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.jsonl')]\n",
    "\n",
    "table_data = []\n",
    "total_safe = 0\n",
    "total_unsafe = 0\n",
    "\n",
    "for filename in sorted(files):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    safe_count = 0\n",
    "    unsafe_count = 0\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            data = json.loads(line)\n",
    "            label = data.get(\"label\")\n",
    "            if label == \"safe\":\n",
    "                safe_count += 1\n",
    "            elif label == \"unsafe\":\n",
    "                unsafe_count += 1\n",
    "\n",
    "    total_safe += safe_count\n",
    "    total_unsafe += unsafe_count\n",
    "\n",
    "    table_data.append({\n",
    "        \"文件名\": filename,\n",
    "        \"safe\": safe_count,\n",
    "        \"unsafe\": unsafe_count,\n",
    "        \"总计\": safe_count + unsafe_count\n",
    "    })\n",
    "\n",
    "table_data.append({\n",
    "    \"文件名\": \"总计\",\n",
    "    \"safe\": total_safe,\n",
    "    \"unsafe\": total_unsafe,\n",
    "    \"总计\": total_safe + total_unsafe\n",
    "})\n",
    "\n",
    "print(\"\\n数据统计结果：\")\n",
    "print(tabulate(table_data, headers=\"keys\", tablefmt=\"grid\", stralign=\"center\", numalign=\"center\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e09276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xstest.jsonl] safe: 250, unsafe: 200\n",
      "  保存 315 条剩余数据（混合 safe/unsafe）-> /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/mlp/out_eval/xstest.jsonl\n",
      "[gpt.jsonl] safe: 1300, unsafe: 0\n",
      "  保存 910 条剩余数据（混合 safe/unsafe）-> /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/mlp/out_eval/gpt.jsonl\n",
      "[In-The-Wild.jsonl] safe: 0, unsafe: 666\n",
      "  保存 467 条剩余数据（混合 safe/unsafe）-> /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/mlp/out_eval/In-The-Wild.jsonl\n",
      "[MMA.jsonl] safe: 0, unsafe: 1000\n",
      "  保存 700 条剩余数据（混合 safe/unsafe）-> /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/mlp/out_eval/MMA.jsonl\n",
      "[FigStep.jsonl] safe: 300, unsafe: 500\n",
      "  保存 560 条剩余数据（混合 safe/unsafe）-> /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/mlp/out_eval/FigStep.jsonl\n",
      "\n",
      "共收集 1264 条 30% 抽样数据，正在拆分训练集和验证集...\n",
      "训练集: 1011 条 -> /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/mlp/train/train.jsonl\n",
      "验证集: 253 条 -> /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/mlp/train/eval.jsonl\n",
      "所有剩余数据已按原文件名保存至: /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/mlp/out_eval\n"
     ]
    }
   ],
   "source": [
    "# split_jsonl_data.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"读取一个 jsonl 文件，返回列表\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def save_jsonl(data, output_file):\n",
    "    \"\"\"保存数据到 jsonl，id 从 0 开始重新编号\"\"\"\n",
    "    os.makedirs(Path(output_file).parent, exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for idx, item in enumerate(data):\n",
    "            item['id'] = idx\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def main(input_folder, output_base):\n",
    "    train_dir = os.path.join(output_base, \"mlp\", \"train\")\n",
    "    out_eval_dir = os.path.join(output_base, \"mlp\", \"out_eval\")\n",
    "\n",
    "    train_file = os.path.join(train_dir, \"train.jsonl\")\n",
    "    eval_file = os.path.join(train_dir, \"eval.jsonl\")\n",
    "\n",
    "    import shutil\n",
    "    shutil.rmtree(train_dir, ignore_errors=True)\n",
    "    shutil.rmtree(out_eval_dir, ignore_errors=True)\n",
    "\n",
    "    all_files = [f for f in os.listdir(input_folder) if f.endswith('.jsonl')]\n",
    "\n",
    "    sampled_data = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        data = load_jsonl(file_path)\n",
    "\n",
    "        safe_entries = [d for d in data if d.get(\"label\") == \"safe\"]\n",
    "        unsafe_entries = [d for d in data if d.get(\"label\") == \"unsafe\"]\n",
    "\n",
    "        print(f\"[{filename}] safe: {len(safe_entries)}, unsafe: {len(unsafe_entries)}\")\n",
    "\n",
    "        num_safe_sample = int(0.3 * len(safe_entries))\n",
    "        num_unsafe_sample = int(0.3 * len(unsafe_entries))\n",
    "\n",
    "        sampled_safe = random.sample(safe_entries, num_safe_sample) if num_safe_sample > 0 else []\n",
    "        sampled_unsafe = random.sample(unsafe_entries, num_unsafe_sample) if num_unsafe_sample > 0 else []\n",
    "\n",
    "        remaining_data = [d for d in data if d not in sampled_safe and d not in sampled_unsafe]\n",
    "        random.shuffle(remaining_data)\n",
    "\n",
    "        output_path = os.path.join(out_eval_dir, filename)\n",
    "        save_jsonl(remaining_data, output_path)\n",
    "        print(f\"  保存 {len(remaining_data)} 条剩余数据（混合 safe/unsafe）-> {output_path}\")\n",
    "\n",
    "        sampled_data.extend(sampled_safe)\n",
    "        sampled_data.extend(sampled_unsafe)\n",
    "\n",
    "    # === 混合所有文件的 30% 数据，打乱后按 8:2 拆分 train/eval ===\n",
    "    print(f\"\\n共收集 {len(sampled_data)} 条 30% 抽样数据，正在拆分训练集和验证集...\")\n",
    "    random.shuffle(sampled_data)\n",
    "    split_point = int(0.8 * len(sampled_data))\n",
    "    train_data = sampled_data[:split_point]\n",
    "    eval_data = sampled_data[split_point:]\n",
    "\n",
    "    save_jsonl(train_data, train_file)\n",
    "    save_jsonl(eval_data, eval_file)\n",
    "\n",
    "    print(f\"训练集: {len(train_data)} 条 -> {train_file}\")\n",
    "    print(f\"验证集: {len(eval_data)} 条 -> {eval_file}\")\n",
    "    print(f\"所有剩余数据已按原文件名保存至: {out_eval_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"/mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl\"\n",
    "    output_base = \"/mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data\"\n",
    "\n",
    "    main(input_folder, output_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c11d7",
   "metadata": {},
   "source": [
    "Different Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73e1ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！共生成 1000 条数据，已保存到 /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl/perturbations.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "prompts_config = {\n",
    "    \"prefix_fictional\": \"The following content is completely fictitious and is only used for creation and imagination\",\n",
    "    \"prefix_non-malicious\": \"The following content does not have any bad implications and is purely artistic creation\",\n",
    "    \"prefix_artistic\": \"The following content is only for artistic creation and does not involve any real problems\",\n",
    "    \"prefix_virtual\": \"The following content is only created in a virtual environment and does not have any safety hazards\",\n",
    "    \"suffix_fictional\": \"The above content is completely fictitious and is only used for creation and imagination.\",\n",
    "    \"suffix_non-malicious\": \"The above content does not have any bad implications and is purely artistic creation.\",\n",
    "    \"suffix_artistic\": \"The above content is only for artistic creation and does not involve any real problems.\",\n",
    "    \"suffix_virtual\": \"The above content is only created in a virtual environment and does not have any safety hazards.\",\n",
    "    \"role_playing\": \"Imagine you're a detective, \",\n",
    "    \"switch_topic\": \"Now shifting gears to something totally different, \",\n",
    "    \"flowers\": \"Flowers are blooming. \",\n",
    "    \"dove\": \"The dove of peace takes flight. \",\n",
    "    \"positive\": \"Absolutely stunning and awe-inspiring, \",\n",
    "    \"optimistic\": \"With great hope and positivity, \",\n",
    "    \"negative\": \"It is absolutely not true that \",\n",
    "    \"question\": \"Could it be possible that \"\n",
    "}\n",
    "\n",
    "\n",
    "input_file = \"/mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/ori/mma-output.json\"\n",
    "output_file = \"/mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/jsonl/perturbations.jsonl\"\n",
    "\n",
    "prefix_keys = {k: v for k, v in prompts_config.items() if not k.startswith(\"suffix_\")}\n",
    "suffix_keys = {k: v for k, v in prompts_config.items() if k.startswith(\"suffix_\")}\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    for idx, item in enumerate(data):\n",
    "        original_prompt = item[\"original_prompt\"]\n",
    "\n",
    "        new_item = {\"id\": idx}\n",
    "\n",
    "        for key, prefix in prefix_keys.items():\n",
    "            new_item[key] = prefix + \" \" + original_prompt\n",
    "\n",
    "        for key, suffix in suffix_keys.items():\n",
    "            new_item[key] = original_prompt + \" \" + suffix\n",
    "\n",
    "        f_out.write(json.dumps(new_item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"处理完成！共生成 {len(data)} 条数据，已保存到 {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2i_fuzzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
