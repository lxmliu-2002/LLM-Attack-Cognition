# python extra_hs.py --input /mnt/sdc/user_workspace/liuxiuming/Projects/ant/LLM-Attack-Cognition/data/mlp/train/train.jsonl --model_path /mnt/sdb/models/Qwen/Qwen3-4B 

import os
import json
import argparse
import numpy as np
from pathlib import Path
import torch

from utils import Qwen_LLM, Llama_LLM


def detect_model_type(model_path):
    """
    æ ¹æ®æ¨¡å‹è·¯å¾„åˆ¤æ–­æ˜¯ Qwen è¿˜æ˜¯ Llama æ¨¡å‹
    """
    model_path = model_path.lower()
    if "qwen" in model_path:
        return "qwen"
    elif "llama" in model_path:
        return "llama"
    else:
        raise ValueError(f"æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹: {model_path}")


def load_jsonl(file_path):
    """è¯»å– jsonl æ–‡ä»¶ï¼Œè¿”å› prompt åˆ—è¡¨å’ŒåŸå§‹æ•°æ®"""
    prompts = []
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                prompts.append(item.get("prompt", ""))
                data.append(item)
    return prompts, data


def main():
    parser = argparse.ArgumentParser(description="æå– JSONL ä¸­ prompt çš„éšè—çŠ¶æ€")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥ JSONL æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True, help="ä¿å­˜è·¯å¾„")

    args = parser.parse_args()

    input_file = args.input
    model_path = args.model_path
    output_dir = args.output_dir

    if not os.path.exists(input_file):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")

    model_type = detect_model_type(model_path)
    print(f"ğŸ” æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type.upper()}")

    if model_type == "qwen":
        llm = Qwen_LLM(model_path=model_path, device="auto", torch_dtype=torch.bfloat16)
    else:
        llm = Llama_LLM(model_path=model_path, device="auto", torch_dtype=torch.bfloat16)

    model_name = Path(model_path).name
    model_name = model_name.replace("-", "_").replace(".", "_")

    print(f"ğŸ“„ æ­£åœ¨åŠ è½½ {input_file} ...")
    prompts, raw_data = load_jsonl(input_file)

    print(f"âœ… å…±åŠ è½½ {len(prompts)} æ¡ prompt")

    print("ğŸ§  æ­£åœ¨æå–éšè—çŠ¶æ€...")
    hidden_states = llm.extract_hidden_states(
        texts=prompts,
        layer_range=None,  # None ä»£è¡¨æ‰€æœ‰å±‚
        mode="last_token"
    )

    os.makedirs(output_dir, exist_ok=True)

    input_filename = Path(input_file).stem
    output_file = f"{model_name}_{input_filename}.npy"
    output_path = os.path.join(output_dir, output_file)

    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜éšè—çŠ¶æ€åˆ°: {output_path}")
    np.save(output_path, hidden_states)

    print(f"ğŸ‰ æå–å®Œæˆï¼å…± {len(prompts)} æ¡æ•°æ®ï¼Œä¿å­˜è‡³:\n  {output_path}")



if __name__ == "__main__":
    main()